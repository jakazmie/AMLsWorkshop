{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Lab 2 - Model Deplyoment \n\nIn this lab you will deploy a trained model to containers using an Azure Container Instance and and Azure Kubernetes Service using the Azure Machine Learning SDK.\n\n![AML Arch](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/amlarch.png)\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Verify AML SDK Installed\n# view version history at https://pypi.org/project/azureml-sdk/#history \nimport azureml.core\nprint(\"SDK Version:\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Connect to the workspace"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Read the workspace config from file\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Get the lab datasets\n\nThe datasets have been downloaded in Lab 1 into the local folder.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\n# Create a temporary folder to store locally relevant content for this notebook\ndatasetsFolderName = '../datasets'\n# os.makedirs(datasetsFolderName, exist_ok=True)\n# print('Content files will be saved to {0}'.format(datasetsFolderName))\n\n# filesToDownload = ['UsedCars_Clean.csv', 'UsedCars_Affordability.csv']\n\n# for fileToDownload in filesToDownload:\n#  downloadCommand = 'wget -O ''{0}/{1}'' ''https://databricksdemostore.blob.core.windows.net/data/aml-labs/{1}'''.format(datasetsFolderName, fileToDownload)\n#  print(downloadCommand)\n#  os.system(downloadCommand)\n  \n# List the downloaded files\nos.listdir(datasetsFolderName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a simple model locally and register it with *Model Registry*\n\nThis lab builds upon the lessons learned in the previous lab, but is designed to be self contained. As such we will repeat model training. After training is completed, the trained model will be registered in *Model Registry*."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a helper method that will train, score and register the classifier using different settings\nimport numpy as np\n\nfrom azureml.core.model import Model \nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\nfrom sklearn import linear_model \nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport pickle\nimport json\n\n\ndef train_eval_register_model(ws, experiment_name, model_name, full_X, full_Y,training_set_percentage):\n\n    # start a training run by defining an experiment\n    myexperiment = Experiment(ws, experiment_name)\n    run = myexperiment.start_logging()\n\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n\n    # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=1)\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    print(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\n\n    # Log the training metrics to Azure Machine Learning service run history\n    run.log(\"Training_Set_Percentage\", training_set_percentage)\n    run.log(\"Accuracy\", score)\n\n    # Serialize the model to a pickle file in the outputs folder\n    output_model_path = 'outputs/' + model_name + '.pkl'\n    pickle.dump(clf,open(output_model_path,'wb'))\n    print('Exported model to ', output_model_path)\n\n    # Serialize the scaler as a pickle file in the same folder as the model\n    output_scaler_path = 'outputs/' + 'scaler' + '.pkl'\n    pickle.dump(scaler,open(output_scaler_path,'wb'))\n    print('Exported scaler to ', output_scaler_path)\n\n    # notice for the model_path, we supply the name of the outputs folder without a trailing slash\n    # this will ensure both the model and the scaler get uploaded.\n    registered_model = Model.register(model_path='outputs', model_name=model_name, workspace=ws)\n\n    print(registered_model.name, registered_model.id, registered_model.version, sep = '\\t')\n\n    run.complete()\n\n    return (registered_model, clf, scaler, score, run)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Experiment and Run training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Load our training data set\ndf_affordability = pd.read_csv('../datasets/UsedCars_Affordability.csv', delimiter=',')\n\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\n# Create an experiment, log metrics and register the created model\nexperiment_name = \"usedcars_training_deployment\"\nmodel_name = \"usedcarsmodel\"\ntraining_set_percentage = 0.50\nregistered_model, model, scaler, score, run = train_eval_register_model(ws, \n                                                                        experiment_name, \n                                                                        model_name, \n                                                                        full_X, full_Y, \n                                                                        training_set_percentage)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Download and test version of a model from Azure Machine Learning.\n\nDownload the model registered in the previous step and run a quick test."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Download the model to a local directory\nmodel_name = 'usedcarsmodel'\nscaler_name = 'scaler'\n\nmodel_path = Model.get_model_path(model_name, _workspace=ws)\n\n# Re-load the model\nmodel = pickle.load(open(os.path.join(model_path, model_name + '.pkl'), 'rb'))\nprint(\"Loaded model from:\", os.path.join(model_path, model_name + '.pkl'))\n\n# Re-load the scaler\nscaler = pickle.load(open(os.path.join(model_path, scaler_name + '.pkl'),'rb'))\nprint(\"Loaded scaler from:\", os.path.join(model_path, scaler_name + '.pkl'))\n\n# Run a quick test\nage = 60\nkm = 40000\n\nscaled_input = scaler.transform([[age, km]])\nprediction = model.predict(scaled_input)\nprint(prediction)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create and deploy the container image encapsulating the model\n\nWhen you deploy a model using AML to either ACI or AKS, you are deploying a Docker container encapsulating a trained model, its dependencies, and a web services wrapper around the model. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Conda dependencies environment file.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmycondaenv = CondaDependencies.create(conda_packages=['scikit-learn','numpy','pandas'])\n\nwith open(\"mydeployenv.yml\",\"w\") as f:\n    f.write(mycondaenv.serialize_to_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Review the content of 'yml' file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"mydeployenv.yml\",\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create scoring script.\nWith Azure Machine Learning, you have full control over the logic of the webservice which includes how it loads your model, transforms web service inputs, uses the model for scoring and returns the result. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score.py\n\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model \nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core import Run\nfrom azureml.core import Workspace\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nimport pickle\nfrom sklearn.externals import joblib\n\ndef init():\n    try:\n        # One-time initialization of predictive model and scaler\n        from azureml.core.model import Model\n        \n        global trainedModel   \n        global scaler\n\n        model_name = \"usedcarsmodel\" \n        model_path = Model.get_model_path(model_name)\n        print('Looking for models in: ', model_path)\n\n        trainedModel = pickle.load(open(os.path.join(model_path,'usedcarsmodel.pkl'), 'rb'))\n        \n        scaler = pickle.load(open(os.path.join(model_path,'scaler.pkl'),'rb'))\n\n    except Exception as e:\n        print('Exception during init: ', str(e))\n\ndef run(input_json):     \n    try:\n        inputs = json.loads(input_json)\n\n        #Scale the input\n        scaled_input = scaler.transform(inputs)\n        \n        #Get the scored result\n        prediction = json.dumps(trainedModel.predict(scaled_input).tolist())\n\n    except Exception as e:\n        prediction = str(e)\n    return prediction\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create docker image for deployment\n\nTo create a Container Image, you need four things: the model metadata (as retrieved from Model Registry), the scoring script file, the runtime configuration (defining whether Python or PySpark should be used) and the Conda Dependencies file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage, Image\n\n# Retrieve the model metadata from Model Registry\nmodel_name = 'usedcarsmodel'\nmodel = Model(workspace=ws, name=model_name)\n\n# Define runtime\nruntime = \"python\" \n\n# Define scoring script\ndriver_file = \"score.py\"\n\n# Define conda dependencies\nconda_file = \"mydeployenv.yml\"\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script=driver_file, \n                                                  runtime=runtime, \n                                                  conda_file=conda_file,\n                                                  description=\"Image for used cars predictor\",\n                                                  tags={\"Classifier\": \"Logistic regression\"})\n\nimage = Image.create(name = \"used-car-classifier-image\",\n                     models = [model],\n                     image_config = image_config, \n                     workspace = ws)\n\nimage.wait_for_creation(show_output = True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Deploy the container image to ACI\n\nWith the Container Image  in hand, you are almost ready to deploy to ACI. The next step is to define the size of the VM that ACI will use to run your Container."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice, Webservice\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1, \n    tags = {'name':'Azure ML ACI'}, \n    description = 'This is a deployment of the affordibility predictor.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "At this point you can deploy the image to the webservice to ACI"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice\n\nservice_name = \"usedcarsmlservice-aci\"\nprint(\"Deploying: \", service_name)\naci_service = Webservice.deploy_from_image(deployment_config = aci_config,\n                                           image = image,\n                                           name = service_name,\n                                           workspace = ws)\naci_service.wait_for_deployment(True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test the service\n\nOnce the webservice deployment completes, you can use the returned webservice object to invoke the webservice. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nage = 60\nkm = 40000\ntest_data  = json.dumps([[age,km]])\ntest_data\nresult = aci_service.run(input_data=test_data)\nprint(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Deploy the container image to AKS\n\nOnce you are familiar with the process for deploying a webservice to ACI, you will find the process for deploying to AKS to be similar with one additional step that creates the AKS cluster first."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Provision an AKS cluster \n\nfrom azureml.core.compute import AksCompute, ComputeTarget\nfrom azureml.core.webservice import Webservice, AksWebservice\n\n# Use the default configuration, overriding the default location to a known region that supports AKS\nprov_config = AksCompute.provisioning_configuration(location='westus2')\n\naks_name = 'aks-cluster01' \n\n# Create the cluster\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\n\n# Wait for cluster to be ready\naks_target.wait_for_completion(show_output = True)\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With your AKS cluster ready, now you can deploy your webservice. Once again, you need to provide a configuration for the size of resources allocated from the AKS cluster to run instances of your Container."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the web service configuration (using defaults)\naks_config = AksWebservice.deploy_configuration()\n\naks_service_name ='usedcarsmlservice-aks'\n\naks_service = Webservice.deploy_from_image(\n  workspace=ws, \n  name=aks_service_name, \n  image = image,\n  deployment_target=aks_target\n  )\n\n\naks_service.wait_for_deployment(show_output = True)\nprint(aks_service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test the service\nAs before, you can use the webservice object returned by the deploy_from_model method to invoke your deployed webservice. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nage = 60\nkm = 40000\ntest_data  = json.dumps([[age,km]])\ntest_data\nresult = aks_service.run(input_data=test_data)\nprint(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up\n\nMake sure to remove ACI and AKS deployments. Use Azure Portal to remove *Deployments* and *AKS Compute*."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}