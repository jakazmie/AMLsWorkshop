{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Lab 1 - Training a Machine Learning Model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this lab you will setup the Azure Machine Learning service and use it for tracking training of a `scikit-learn` model.\n\n![AML Arch](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/amlarch.png)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Get the lab datasets\nThe following cell will download the dataset used by this lab. Click into the following cell and use `Shift + Enter` to execute it"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\n# Create a temporary folder to store locally relevant content for this notebook\ndatasetsFolderName = '../datasets'\nos.makedirs(datasetsFolderName, exist_ok=True)\nprint('Content files will be saved to {0}'.format(datasetsFolderName))\n\nfilesToDownload = ['UsedCars_Clean.csv', 'UsedCars_Affordability.csv']\n\nfor fileToDownload in filesToDownload:\n  downloadCommand = 'wget -O ''{0}/{1}'' ''https://databricksdemostore.blob.core.windows.net/data/aml-labs/{1}'''.format(datasetsFolderName, fileToDownload)\n  print(downloadCommand)\n  os.system(downloadCommand)\n  \n#List all downloaded files\nos.listdir(datasetsFolderName)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Content files will be saved to ../datasets\nwget -O ../datasets/UsedCars_Clean.csv https://databricksdemostore.blob.core.windows.net/data/aml-labs/UsedCars_Clean.csv\nwget -O ../datasets/UsedCars_Affordability.csv https://databricksdemostore.blob.core.windows.net/data/aml-labs/UsedCars_Affordability.csv\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "['UsedCars_Clean.csv', 'UsedCars_Affordability.csv']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a simple model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following cell loads the sampled dataset. Use `Shift + Enter` to execute the cell. Take a moment to look at the data loaded into the Pandas Dataframe - it contains data about used cars such as the price (in dollars), age (in years), KM (kilometers driven) and other attributes like weather it is automatic transimission, the number of doors, and the weight."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load the data\n\nimport numpy as np\nimport pandas as pd\nimport os\n\npathToCsvFile = os.path.join(datasetsFolderName, 'UsedCars_Clean.csv')\ndf = pd.read_csv(pathToCsvFile, delimiter=',')\nprint(df)\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "      Price  Age     KM FuelType   HP  MetColor  Automatic    CC  Doors  \\\n0     13500   23  46986   Diesel   90         1          0  2000      3   \n1     13750   23  72937   Diesel   90         1          0  2000      3   \n2     13950   24  41711   Diesel   90         1          0  2000      3   \n3     14950   26  48000   Diesel   90         0          0  2000      3   \n4     13750   30  38500   Diesel   90         0          0  2000      3   \n5     12950   32  61000   Diesel   90         0          0  2000      3   \n6     16900   27  94612   Diesel   90         1          0  2000      3   \n7     18600   30  75889   Diesel   90         1          0  2000      3   \n8     21500   27  19700   Petrol  192         0          0  1800      3   \n9     12950   23  71138   Diesel   69         0          0  1900      3   \n10    20950   25  31461   Petrol  192         0          0  1800      3   \n11    19950   22  43610   Petrol  192         0          0  1800      3   \n12    19600   25  32189   Petrol  192         0          0  1800      3   \n13    21500   31  23000   Petrol  192         1          0  1800      3   \n14    22500   32  34131   Petrol  192         1          0  1800      3   \n15    22000   28  18739   Petrol  192         0          0  1800      3   \n16    22750   30  34000   Petrol  192         1          0  1800      3   \n17    17950   24  21716   Petrol  110         1          0  1600      3   \n18    16750   24  25563   Petrol  110         0          0  1600      3   \n19    16950   30  64359   Petrol  110         1          0  1600      3   \n20    15950   30  67660   Petrol  110         1          0  1600      3   \n21    16950   29  43905   Petrol  110         0          1  1600      3   \n22    15950   28  56349   Petrol  110         1          0  1600      3   \n23    16950   28  32220   Petrol  110         1          0  1600      3   \n24    16250   29  25813   Petrol  110         1          0  1600      3   \n25    15950   25  28450   Petrol  110         1          0  1600      3   \n26    17495   27  34545   Petrol  110         1          0  1600      3   \n27    15750   29  41415   Petrol  110         1          0  1600      3   \n28    16950   28  44142   Petrol  110         0          0  1600      3   \n29    17950   30  11090   Petrol  110         1          0  1600      3   \n...     ...  ...    ...      ...  ...       ...        ...   ...    ...   \n1406   8950   70  44850   Petrol  110         1          0  1600      3   \n1407   8250   69  44826   Petrol  110         0          0  1600      5   \n1408   9250   80  44444   Petrol  110         1          0  1600      3   \n1409   7900   75  43720   Petrol  110         1          0  1600      5   \n1410   8500   78  43622   Petrol   86         1          0  1300      4   \n1411   7950   76  43532   Petrol  110         0          0  1600      5   \n1412   9950   69  42800   Petrol  110         1          0  1600      3   \n1413   8750   74  42317   Petrol  107         1          1  1600      5   \n1414   7500   80  42186   Petrol  110         1          0  1600      3   \n1415   6950   72  42000   Petrol  110         1          0  1600      3   \n1416   8950   79  40093   Petrol  110         0          0  1600      5   \n1417   8750   79  39800   Petrol  107         0          1  1600      3   \n1418   7750   73  39168   Petrol   86         0          0  1300      3   \n1419   8450   75  38945   Petrol  110         1          0  1600      3   \n1420   8150   76  36537   Petrol  110         0          1  1600      4   \n1421   8500   78  36000   Petrol   86         0          1  1300      3   \n1422   7600   78  36000   Petrol  110         1          0  1600      3   \n1423   7950   80  35821   Petrol   86         0          1  1300      3   \n1424   7750   73  34717   Petrol   86         0          0  1300      3   \n1425   7950   80  34000   Petrol   86         1          0  1300      4   \n1426   9950   78  30964   Petrol  110         0          1  1600      3   \n1427   8950   71  29000   Petrol   86         1          1  1300      3   \n1428   8450   72  26000   Petrol   86         0          0  1300      3   \n1429   8950   78  24000   Petrol   86         1          1  1300      5   \n1430   8450   80  23000   Petrol   86         0          0  1300      3   \n1431   7500   69  20544   Petrol   86         1          0  1300      3   \n1432  10845   72  19000   Petrol   86         0          0  1300      3   \n1433   8500   71  17016   Petrol   86         0          0  1300      3   \n1434   7250   70  16916   Petrol   86         1          0  1300      3   \n1435   6950   76      1   Petrol  110         0          0  1600      5   \n\n      Weight  \n0       1165  \n1       1165  \n2       1165  \n3       1165  \n4       1170  \n5       1170  \n6       1245  \n7       1245  \n8       1185  \n9       1105  \n10      1185  \n11      1185  \n12      1185  \n13      1185  \n14      1185  \n15      1185  \n16      1185  \n17      1105  \n18      1065  \n19      1105  \n20      1105  \n21      1170  \n22      1120  \n23      1120  \n24      1120  \n25      1120  \n26      1120  \n27      1120  \n28      1120  \n29      1120  \n...      ...  \n1406    1050  \n1407    1075  \n1408    1050  \n1409    1070  \n1410    1000  \n1411    1070  \n1412    1050  \n1413    1100  \n1414    1050  \n1415    1050  \n1416    1114  \n1417    1080  \n1418    1015  \n1419    1050  \n1420    1075  \n1421    1045  \n1422    1050  \n1423    1015  \n1424    1015  \n1425    1000  \n1426    1080  \n1427    1045  \n1428    1015  \n1429    1065  \n1430    1015  \n1431    1025  \n1432    1015  \n1433    1015  \n1434    1015  \n1435    1114  \n\n[1436 rows x 10 columns]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to try and build a model that can answer the question \"Can I afford a car that is X months old and has Y kilometers on it, given I have $12,000 to spend?\". We will engineer the label for affordable. Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add the affordable feature\n\ndf['Affordable'] = np.where(df['Price']<12000, 1, 0)\ndf_affordability = df[[\"Age\",\"KM\", \"Affordable\"]]\nprint(df_affordability)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "      Age     KM  Affordable\n0      23  46986           0\n1      23  72937           0\n2      24  41711           0\n3      26  48000           0\n4      30  38500           0\n5      32  61000           0\n6      27  94612           0\n7      30  75889           0\n8      27  19700           0\n9      23  71138           0\n10     25  31461           0\n11     22  43610           0\n12     25  32189           0\n13     31  23000           0\n14     32  34131           0\n15     28  18739           0\n16     30  34000           0\n17     24  21716           0\n18     24  25563           0\n19     30  64359           0\n20     30  67660           0\n21     29  43905           0\n22     28  56349           0\n23     28  32220           0\n24     29  25813           0\n25     25  28450           0\n26     27  34545           0\n27     29  41415           0\n28     28  44142           0\n29     30  11090           0\n...   ...    ...         ...\n1406   70  44850           1\n1407   69  44826           1\n1408   80  44444           1\n1409   75  43720           1\n1410   78  43622           1\n1411   76  43532           1\n1412   69  42800           1\n1413   74  42317           1\n1414   80  42186           1\n1415   72  42000           1\n1416   79  40093           1\n1417   79  39800           1\n1418   73  39168           1\n1419   75  38945           1\n1420   76  36537           1\n1421   78  36000           1\n1422   78  36000           1\n1423   80  35821           1\n1424   73  34717           1\n1425   80  34000           1\n1426   78  30964           1\n1427   71  29000           1\n1428   72  26000           1\n1429   78  24000           1\n1430   80  23000           1\n1431   69  20544           1\n1432   72  19000           1\n1433   71  17016           1\n1434   70  16916           1\n1435   76      1           1\n\n[1436 rows x 3 columns]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to train a Logistic Regression model in Azure Databricks. This type of model requires us to standardize the scale of our training features Age and KM, so we use the `StandardScaler` from Scikit-Learn to transform these features so that they have values centered with a mean around 0 (mostly between -2.96 and 1.29). Select Step 3 and execute the code. Observe the difference in min and max values between the un-scaled and scaled Dataframes. When we use Sci-Kit Learn, these models are trained on the driver node. Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Scale the numeric feature values\n\nX = df_affordability[[\"Age\", \"KM\"]].values\ny = df_affordability[[\"Affordable\"]].values\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X.astype(float))\n\nprint(pd.DataFrame(X).describe().round(2))\nprint(pd.DataFrame(X_scaled).describe().round(2))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "             0          1\ncount  1436.00    1436.00\nmean     55.95   68533.26\nstd      18.60   37506.45\nmin       1.00       1.00\n25%      44.00   43000.00\n50%      61.00   63389.50\n75%      70.00   87020.75\nmax      80.00  243000.00\n             0        1\ncount  1436.00  1436.00\nmean     -0.00     0.00\nstd       1.00     1.00\nmin      -2.96    -1.83\n25%      -0.64    -0.68\n50%       0.27    -0.14\n75%       0.76     0.49\nmax       1.29     4.65\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Train the model by fitting a LogisticRegression against the scaled input features (X_scaled) and the labels (y). Execute the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit a Logistic Regression\n\nfrom sklearn import linear_model\n# Create a linear model for Logistic Regression\nclf = linear_model.LogisticRegression(C=1)\n\n# Flatten labels\ny = np.ravel(y)\n\n# we create an instance of Classifier and fit the data.\nclf.fit(X_scaled, y)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Try prediction - if you set the age to 60 months and km to 40,000, does the model predict you can afford the car? Execute the cell and find out."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test the trained model's prediction\n\nage = 60\nkm = 40000\n\nscaled_input = scaler.transform([[age, km]])\nprediction = clf.predict(scaled_input)\n\nprint(\"Can I afford a car that is {} month(s) old with {} KM's on it?\".format(age,km))\nprint(\"Yes (1)\" if prediction[0] == 1 else \"No (0)\")",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Can I afford a car that is 60 month(s) old with 40000 KM's on it?\nYes (1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's get a sense for how accurate the model is. Execute the following cell. What was your model's accuracy?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Measure the model's performance\n\nscaled_inputs = scaler.transform(X.astype(float))\npredictions = clf.predict(scaled_inputs)\nprint(predictions)\n\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y, predictions)\nprint(\"Model Accuracy: {}\".format(score.round(3)))",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0 0 0 ... 1 1 1]\nModel Accuracy: 0.926\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The key hyperparameter of logistic regression is regularization strength. To be precise it is the inverse of regularization strength in `sklearn`. In the next cell, you define a method that allows you to experiment with different values of `C`. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a method to experiment with different values of C\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\ndef train_eval_model(full_X, full_Y,training_set_percentage, C):\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n    \n    # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=C, solver='lbfgs')\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    return (clf, score)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use Azure Machine Learning to log performance metrics\nIn the steps that follow, you will train multiple models using different values of C and observe the impact on performance (accuracy). Each time you create a new model, you are executing a Run in the terminology of Azure Machine Learning service. In this case, you will create one Experiment and execute multiple Runs within it, each with different value of C."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Execute the following cell to quickly verify you have the Azure Machine Learning SDK installed on your cluster. If you get a version number back without error, you are ready to proceed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Verify AML SDK Installed\n\nimport azureml.core\nprint(\"SDK Version:\", azureml.core.VERSION)\n\n# import the Workspace class \nfrom azureml.core import Workspace",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SDK Version: 1.0.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "All Azure Machine Learning entities are organized within a Workspace. You can create an AML Workspace in the Azure Portal, but as the code in the following cell shows, you can also create a Workspace directly from code. Set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments. Execute Step 9. You will be prompted to log in to your Azure Subscription by the command output."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a workspace\n\n#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"<your subscription>\"\n\n#Provide values for the Resource Group and Workspace that will be created\nresource_group = \"<your resource group>\"\nworkspace_name = \"<your workspace name>\"\nworkspace_region = \"<your region>\"\n\nws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nprint(\"Workspace Provisioning complete.\")\nws.get_details()\nws.write_config('../')",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UserErrorException",
          "evalue": "Workspace name must be between 3 and 33 characters long. Its first character has to be alphanumeric, and the rest may contain hyphens and underscores.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7c32a43f7060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mresource_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_group\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace_region\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     exist_ok = True)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Workspace Provisioning complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/azureml/core/workspace.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(name, auth, subscription_id, resource_group, location, create_resource_group, friendly_name, storage_account, key_vault, app_insights, container_registry, exist_ok)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# Checking the validity of the workspace name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mcheck_valid_resource_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Workspace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;31m# TODO: Checks if the workspace already exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/azureml/_base_sdk_common/common.py\u001b[0m in \u001b[0;36mcheck_valid_resource_name\u001b[0;34m(resource_name, resource_type)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUserErrorException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_valid_workspace_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mUserErrorException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUserErrorException\u001b[0m: Workspace name must be between 3 and 33 characters long. Its first character has to be alphanumeric, and the rest may contain hyphens and underscores."
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a workspace\n\n#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"952a710c-8d9c-40c1-9fec-f752138cc0b3\"\n\n#Provide values for the Resource Group and Workspace that will be created\nresource_group = \"jkamlslabs\"\nworkspace_name = \"jkamlslabs\"\nworkspace_region = \"eastus\"\n\nws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nprint(\"Workspace Provisioning complete.\")\nws.get_details()\nws.write_config('../')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To begin capturing metrics, you must first create an Experiment and then call `start_logging()` on that Experiment. The return value of this call is a Run. This root run can have other child runs. When you are finished with an experiment run, use `complete()` to close out the root run. Execute the following cell to train four different models using differing amounts of training data and log the results to Azure Machine Learning."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an experiment and log metrics for multiple training runs\n\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"usedcars_training_local\")\nroot_run = myexperiment.start_logging()\n\ntraining_set_percentage = 0.25\nC = 2\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\nprint(\"With C={}, model accuracy reached {}\".format(C, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"C\", C)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\nC = 1\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\nprint(\"With C={}, model accuracy reached {}\".format(C, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"C\", C)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\nC = 0.75\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\nprint(\"With C={}, model accuracy reached {})\".format(C, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"C\", C)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\nC = 0.5\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\nprint(\"With C={}, model accuracy reached {})\".format(C, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"C\", C)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\n# Close out the experiment\nroot_run.complete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that you have captured history for various runs, you can review the runs. You could use the Azure Portal for this - go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment. However, in this case we will use the AML SDK to query for the runs. Execute the following cell to view the runs and their status."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Review captured runs\n# Go to the Azure Portal, find your Azure Machine Learning Workspace, select Experiments and select the UsedCars_Experiment\n\n# You can also query the run history using the SDK.\n# The following command lists all of the runs for the experiment\nruns = [r for r in root_run.get_children()]\nprint(runs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train remotely using Azure ML Compute\n\nUp until now, all of your training was executed locally on the same machine running Jupyter. Now you will execute the same logic targeting a remote Azure ML Compute, which you will provision from code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Connect to workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an Azure ML Compute cluster\n\n# Create Azure ML cluster\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncluster_name = \"cpu-bai-cluster\"\ncluster_min_nodes = 1\ncluster_max_nodes = 3\nvm_size = \"STANDARD_DS11_V2\"\n\nif cluster_name in ws.compute_targets:\n    compute_target = ws.compute_targets[cluster_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n    else:\n        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\nelse:\n    print('Creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With your cluster ready, you need to upload the training data to the default DataStore for your AML Workspace (which uses Azure Storage). "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Upload the dataset to the DataStore\n\nds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\nds.upload(src_dir='../datasets', target_path='used_cars', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, you will need to create a training script that is similar to the code you have executed locally to train the model. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './script'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import linear_model \nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom azureml.core import Run\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the training set percentage to use\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--training-set-percentage', type=float, dest='training_set_percentage', default=0.25, help='percentage of dataset to use for training')\nparser.add_argument('--C', type=float, dest='C', default=1, help='regularization')\nargs = parser.parse_args()\n\ndata_folder = os.path.join(args.data_folder, 'used_cars')\nprint('Data folder:', data_folder)\ndata_csv_path = os.path.join(data_folder, 'UsedCars_Clean.csv')\nprint('Path to CSV file dataset:' + data_csv_path)\n\n# Load the data\n#df = pd.read_csv('UsedCars_Clean.csv', delimiter=',')\ndf = pd.read_csv(data_csv_path)\ndf['Affordable'] = np.where(df['Price']<12000, 1, 0)\ndf_affordability = df[[\"Age\",\"KM\", \"Affordable\"]]\n\n\n# Now experiment with different training subsets\nfrom sklearn.model_selection import train_test_split\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\ndef train_eval_model(full_X, full_Y,training_set_percentage, C):\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n    \n    # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=C, solver='lbfgs')\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    return (clf, score)\n\n# Acquire the current run\nrun = Run.get_context()\n\n\ntraining_set_percentage = args.training_set_percentage\nC = args.C\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage, C)\nprint(\"With C={}, model accuracy reached {})\".format(C, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"C\", C)\nrun.log(\"Accuracy\", score)\n\n\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=model, filename='outputs/model.pkl')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create an estimator that descrives the configuration of the job that will execute your model training script."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create estimator\n#############################\nfrom azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.as_mount(),\n    '--training-set-percentage': 0.3,\n    '--C': 2\n}\n\nest_config = Estimator(source_directory=script_folder,\n                       script_params=script_params,\n                       compute_target=compute_target,\n                       entry_script='train.py',\n                       conda_packages=['scikit-learn', 'pandas'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Submit the job using the submit() method of the Experiment object. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  Execute the estimator job\n#####################################\n\n# Create new experiment\nfrom azureml.core import Experiment\nexperiment_name = \"usedcars_training_amlcompute\"\nexp = Experiment(workspace=ws, name=experiment_name)\n\nrun = exp.submit(config=est_config)\nrun\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can monitor the job through Azure Portal or using AML Jupyter Widget."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Poll for job status\nrun.wait_for_completion(show_output=True)  # value of True will display a verbose, streaming log\n\n# Examine the recorded metrics from the run\nprint(run.get_metrics())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "01 model training",
    "notebookId": 863281121960369
  },
  "nbformat": 4,
  "nbformat_minor": 1
}