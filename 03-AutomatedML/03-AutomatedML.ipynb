{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Lab 3 - Model Training with AutomatedML\n\nIn this lab you will us the automated machine learning (Auto ML) capabilities within the Azure Machine Learning service to automatically train multiple models with varying algorithms and hyperparameters, select the best performing model and register that model.\n\n![AutomatedML](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/automated-machine-learning.png)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Verify AML SDK Installed\n# view version history at https://pypi.org/project/azureml-sdk/#history \nimport azureml.core\nprint(\"SDK Version:\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Connect to the workspace"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Read the workspace config from file\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a model using AutomatedML\n\nThis lab builds upon the lessons learned in the previous lab, but is self contained so you work thru this lab without having to run a previous lab.\n\nTo train a model using AutoML you need only provide a configuration for AutoML that defines items such as the type of model (classification or regression), the performance metric to optimize, exit criteria in terms of max training time and iterations and desired performance, any algorithms that should not be used, and the path into which to output the results. This configuration is specified using the AutomMLConfig class, which is then used to drive the submission of an experiment via experiment.submit. When AutoML finishes the parent run, you can easily get the best performing run and model from the returned run object by using run.get_output().\n\n### Create/Get Azure ML Compute cluster\n\nWe will run the AutomatedML job in parallel on AzureML Compute cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an Azure ML Compute cluster\n\n# Create Azure ML cluster\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncluster_name = \"cpu-bai-cluster\"\ncluster_min_nodes = 1\ncluster_max_nodes = 3\nvm_size = \"STANDARD_DS11_V2\"\n\nif cluster_name in ws.compute_targets:\n    compute_target = ws.compute_targets[cluster_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n    else:\n        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\nelse:\n    print('Creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Get Data script\n\nIf you are using a remote compute to run your Automated ML experiments, the data fetch must be wrapped in a separate python script that implements get_data() function. This script is run on the remote compute where the automated ML experiment is run. get_data() eliminates the need to fetch the data over the wire for each iteration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nproject_folder = './project'\nscript_name = 'get_data.py'\nos.makedirs(project_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $project_folder/get_data.py\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\n\ndef get_data():\n    # Load bottleneck features\n    data_folder = os.environ[\"AZUREML_DATAREFERENCE_workspaceblobstore\"]\n    file_name = os.path.join(data_folder, 'UsedCars_Affordability.csv')\n    \n    print(\"Data folder:\", data_folder)\n    print(\"Dataset:\", file_name)\n    print(\"Data folder content:\", os.listdir(data_folder))\n    \n    df_affordability = pd.read_csv(file_name, delimiter=',')\n\n    features = df_affordability[[\"Age\", \"KM\"]]\n    labels = df_affordability[[\"Affordable\"]]\n\n        \n    # Split the data into training and validation partitions   \n    train_X, test_X, train_Y, test_Y  = train_test_split(features, labels,\n                                                               test_size=0.2,\n                                                               shuffle=True)\n        # Flatten labels\n    train_Y = np.ravel(train_Y)\n    test_Y = np.ravel(test_Y)\n    \n    # Convert to float\n    train_X = train_X.astype(float)\n    test_X = test_X.astype(float)\n        \n\n    return {'X': train_X, 'y': train_Y, 'X_valid': test_X, 'y_valid': test_Y}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure datastore and data reference\n\nThe training files have been uploaded to the workspace's default datastore during the previous step. We will download the files onto the nodes of the cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\nfrom azureml.core.runconfig import DataReferenceConfiguration\n\nds = ws.get_default_datastore()\nprint(\"Using the default datastore for training data: \")\nprint(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n\ndr = DataReferenceConfiguration(datastore_name=ds.name, \n                   path_on_datastore='used_cars', \n                   path_on_compute='used_cars',\n                   mode='download', # download files from datastore to compute target\n                   overwrite=True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Docker run configuration\nWe will run Automated ML jobs in a custom docker image that will include dependencies required by get_data() script."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Run\nfrom azureml.core import ScriptRunConfig\n\n# create a new RunConfig object\nrun_config = RunConfiguration(framework=\"python\")\n\n# Azure ML Compute cluster for Automated ML jobs require docker.\nrun_config.environment.docker.enabled = True\n\n# Set compute target to BAI cluster\nrun_config.target = compute_target.name\n\n# Set data references\nrun_config.data_references = {ds.name: dr}\n\n# specify packages required by get_data\n# run_config.environment.python.conda_dependencies = \\\n#   CondaDependencies.create(conda_packages=['h5py'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n### Configure Automated ML run.\n\nAutomated ML runs can be controlled using a number of configuration parameters. \n\n\n|Property|Description|\n|-|-|\n|**task**|classification or regression|\n|**primary_metric**|This is the metric that you want to optimize.<br> Classification supports the following primary metrics <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>balanced_accuracy</i><br><i>average_precision_score_weighted</i><br><i>precision_score_weighted</i>|\n|**max_time_sec**|Time limit in seconds for each iteration|\n|**iterations**|Number of iterations. In each iteration Auto ML trains a specific pipeline with the data|\n|**n_cross_validations**|Number of cross validation splits|\n|**concurrent_iterations**|Max number of iterations that would be executed in parallel. |\n|**preprocess**| *True/False* <br>Setting this to *True* enables Auto ML to perform preprocessing <br>on the input to handle *missing data*, and perform some common *feature extraction*|\n|**max_cores_per_iteration**| Indicates how many cores on the compute target would be used to train a single pipeline.<br> Default is *1*, you can set it to *-1* to use all cores|\n|**exit_score**|*double* value indicating the target for *primary_metric*. <br>Once the target is surpassed the run terminates.|\n|**blacklist_algos**|*List* of *strings* indicating machine learning algorithms for AutoML to avoid in this run.<br><br> Allowed values for **Classification**<br><i>LogisticRegression</i><br><i>SGDClassifierWrapper</i><br><i>NBWrapper</i><br><i>BernoulliNB</i><br><i>SVCWrapper</i><br><i>LinearSVMWrapper</i><br><i>KNeighborsClassifier</i><br><i>GradientBoostingClassifier</i><br><i>DecisionTreeClassifier</i><br><i>RandomForestClassifier</i><br><i>ExtraTreesClassifier</i><br><i>LightGBMClassifier</i><br><br>Allowed values for **Regression**<br><i>ElasticNet<i><br><i>GradientBoostingRegressor<i><br><i>DecisionTreeRegressor<i><br><i>KNeighborsRegressor<i><br><i>LassoLars<i><br><i>SGDRegressor<i><br><i>RandomForestRegressor<i><br><i>ExtraTreesRegressor<i>|\n|**path**|Relative path to the project folder. AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|\n    \nFor the optimal performance of `AutomatedML` it is recommended to run at least 100 iterations. Due to the lab's time constraints we will only run 50 iterations. We will also limit a number of alogirthms tried using the `blacklist_algos` parameter."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.run import AutoMLRun\nimport logging\n\n\nautoml_config = AutoMLConfig(run_configuration = run_config,\n                             task = 'classification',\n                             debug_log = 'automl_errors.log',\n                             primary_metric = 'accuracy',\n                             iterations = 50,\n                             max_concurrent_iterations = cluster_max_nodes,\n                             max_cores_per_iteration = 1,\n                             preprocess = False,\n                             experiment_exit_score = 0.98,\n                             #blacklist_models = ['kNN','LinearSVM'],\n                             blacklist_models = ['KNeighborsClassifier','LinearSVMWrapper'],\n                             verbosity = logging.INFO,\n                             path = project_folder,\n                             data_script = os.path.join(project_folder, script_name))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run AutomatedML job."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexperiment_name = \"usedcars_training_automatedml\"\nexp = Experiment(ws, experiment_name)\ntags = {\"Desc\": \"automated ml\"}\nrun = exp.submit(config=automl_config, tags=tags)\nrun",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The call to experiment returns `AutoMLRun` object that can be used to track the run.\n\nSince the call is asynchronous, it reports a **Preparing** or **Running** state as soon as the job is started.\n\nHere is what's happening while you wait:\n\n- **Image creation**: A Docker image is created matching the Python environment specified by the RunConfiguration. The image is uploaded to the workspace. This happens only once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n\n- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. \n\n- **Running**: In this stage, the Automated ML takes over and starts running experiments\n\n\n\nYou can check the progress of a running job in multiple ways: Azure Portal, AML Widgets or streaming logs.\n\n### Monitor the run.\n\nWe will use AML Widget to monitor the run. The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n\nThe widget is asynchronous - it does not block the notebook. You can execute other cells while the widget is running.\n\n**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Cancelling Runs\n\nYou can cancel ongoing remote runs using the `cancel` and `cancel_iteration` functions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Cancel the ongoing experiment and stop scheduling new iterations.\n# run.cancel()\n\n# Cancel iteration 1 and move onto iteration 2.\n# run.cancel_iteration(1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Analyze the run\n\nYou can  use SDK methods to fetch all the child runs and see individual metrics that we log."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\nchildren = list(run.get_children())\nmetricslist = {}\nfor child in children:\n    properties = child.get_properties()\n    metrics = {k: v for k, v in child.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Waiting until the run finishes\n\n`wait_for_complettion` method will block till the run finishes. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Wait until the run finishes.\nrun.wait_for_completion(show_output = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Try the best model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "age = 60\nkm = 4000\n\nprint(best_model.predict( [[age,km]] ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register the best performing model for later use and deployment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# notice the use of the root run (not best_run) to register the best model\nrun.register_model(description='AutoML trained used cars classifier')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}