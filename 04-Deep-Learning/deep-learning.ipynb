{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Lab 4 - Transfer Learning\n\nIn this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n\nDeveloped | Cultivated | Barren\n--------- | ------ | ----------\n![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n\nForested | Grassland | Shrub\n---------| ----------| -----\n![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n\nYou shall employ a machine learning technique called transfer learning. Transfer learning is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n\nAlthough the primary goal of this lab is to understand how to use Azure ML to orchestrate deep learning workflows rather then to dive into Deep Learning techniques, ask the instructor if you want to better understand the approach utilized in the lab.\n\nYou will start by pre-processing training images into a set of powerful features - sometimes referred to as bottleneck features.\n\nTo create bottleneck features you will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n\nAlthough, the pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n\nThe below diagram represents the architecture of our solution.\n\n![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/TLArch.png)\n\nWe will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n\nWe will then use extracted features to train a simple fully connected neural network (the top) that will peform final image classification.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check core SDK version number\nimport azureml.core\nprint(\"SDK version:\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Connect to AML Workspace"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nfrom azureml.core import Workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature Engineering\n\nThe Python script processes an input image dataset into an output bottleneck feature set. The script expects the images to be organized in the below folder structure:\n```\nBarren/\nCultivated/\nDeveloped/\nForest/\nHerbaceous/\nShrub/\n```\n\nThe location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n\nThe script is designed to work with a large number of images. As such it does not load all input images to memory at once. Instead it utilizes a utility function `load_images` to feed the featurizer. The function yields batches of images - as Numpy arrays - preprocessed to the format required by **ResNet50**. \n\nWe will not attempt to run the script on a full dataset in a local environment. It is very computationally intensive and unless you run it in an evironment equipped with a powerful GPU it would be very slow. \n\nHowever, we will demonstrate how to run the script locally using the same small development dataset we used in the previous lab. Running the script locally under the control of Azure ML can be very usefull during development and debugging.\n\nTo process the full dataset we will execute the script on a remote Azure ML Compute equipped with NVidia GPU."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a feature engineering script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './script'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/extract.py\n\nimport os\nimport numpy as np\nimport random\nimport h5py\nfrom tqdm import tqdm\n\nimport tensorflow as tf\n\nimport azureml.contrib.brainwave.models.utils as utils\nfrom azureml.contrib.brainwave.models import QuantizedResnet50\n\n\ndef get_batch(pathnames, batchsize=64):\n    \"\"\"Yield succesive batches of images\"\"\"\n    for i in range(0, len(pathnames), batchsize):\n        yield pathnames[i:i+batchsize]\n        \n\ndef load_images(batch):\n    \"\"\"Return a batch of images as a list of bytes sequences\"\"\"\n    images = []\n    for path in batch:\n        with open(path, 'rb') as f:\n            images.append(f.read())\n    return images\n\ndef create_bottleneck_features():\n    \"\"\"Createl bottleneck features and save them to H5 formatted file\"\"\"\n    img_dir = FLAGS.input_data_dir\n    \n    # Label images \n    \n    # Create the dictionary that maps class names into numeric labels   \n    label_map = {\n        \"Barren\": 0,\n        \"Cultivated\": 1,\n        \"Developed\": 2,\n        \"Forest\": 3,\n        \"Herbaceous\": 4,\n        \"Shrub\": 5}    \n\n    # Create a list of all images in a root folder with associated numeric labels\n    folders = list(label_map.keys())\n    labeled_image_list = [(os.path.join(img_dir, folder, image), label_map[folder]) \n                          for folder in folders \n                          for image in os.listdir(os.path.join(img_dir, folder))\n                              ]\n    # Shuffle the list\n    random.shuffle(labeled_image_list)\n    image_paths, labels = zip(*labeled_image_list)\n    \n    # Build featurizer graph\n    \n    # Convert input images (loaded as bytes sequences) into (224, 224, 3) tensors\n    # with pixel values in Caffe encoding\n    in_images = tf.placeholder(tf.string)\n    image_tensors = utils.preprocess_array(in_images)\n\n    # Create ResNet152 \n    model_path = os.path.expanduser('~/models')\n    resnet = QuantizedResnet50(model_path, is_frozen=True)\n\n    # Import ResNet152 graph\n    features = resnet.import_graph_def(input_tensor=image_tensors)\n    \n    # Generate bottleneck features\n    print(\"Generating bottleneck features\")\n    bottleneck_features = []\n    with tf.Session() as sess:\n        for paths in tqdm(get_batch(image_paths)):\n            image_batch = load_images(paths)\n            result = sess.run([features], feed_dict={in_images: image_batch})\n            result = np.reshape(result[0], (len(result[0]), 2048))\n            bottleneck_features.extend(result)\n        \n    bottleneck_features = np.array(bottleneck_features)\n    print(bottleneck_features.shape)\n        \n    # Save the bottleneck features to HDF5 file\n    filename = FLAGS.file_name\n    output_file = os.path.join(FLAGS.output_data_dir, filename)\n    labels = np.asarray(labels)\n    print(\"Saving bottleneck features to {}\".format(output_file))\n    print(\"   Features: \", bottleneck_features.shape)\n    print(\"   Labels: \", labels.shape)\n    with h5py.File(output_file, \"w\") as hfile:\n        features_dset = hfile.create_dataset('features', data=bottleneck_features)\n        labels_dset = hfile.create_dataset('labels', data=labels)\n    \n    print(\"Done\")\n\nFLAGS = tf.app.flags.FLAGS\n\n# Default global parameters\ntf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\ntf.app.flags.DEFINE_string('input_data_dir', 'aerialtiny', \"Folder with training and validation images\")\ntf.app.flags.DEFINE_string('output_data_dir', 'bottleneck_features', \"A folder for saving bottleneck features\")\ntf.app.flags.DEFINE_string('file_name', 'aerial_bottleneck_resnet50.h5', \"Name of output training file\")\n\n\ndef main(argv=None):\n    print(\"Starting\")\n    print(\"Reading images from:\", FLAGS.input_data_dir)\n    print(\"The output bottleneck file will be saved to:\", FLAGS.output_data_dir)\n\n    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n\n    create_bottleneck_features()\n  \nif __name__ == '__main__':\n    tf.app.run()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Azure ML Managed Compute\n\nWe will use an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n\n# choose a name for your cluster\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-bai-cluster\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n     # For a more detailed view of current AmlCompute status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure Datastores \nThe training images have been uploaded to a public Azure blob storage container. We will register this container as an AML Datastore within our workspace. Before the data prep script runs, the datastore's content - training images - will be copied to the local storage on the compute nodes.\n\nAfter the script completes, its output - the bottleneck features file - will be uploaded by AML to the workspace's default datastore."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nimages_account = 'azureailabs'\nimages_container = 'aerial-small'\ndatastore_name = 'input_images'\n\n# Check if the datastore exists. If not create a new one\ntry:\n    input_ds = Datastore.get(ws, datastore_name)\n    print('Found existing datastore for input images:', input_ds.name)\nexcept:\n    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n                                            container_name=images_container,\n                                            account_name=images_account)\n    print('Creating new datastore for input images')\n\n \n   \nprint(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n\noutput_ds = ws.get_default_datastore()\nprint(\"Using the default datastore for output: \")\nprint(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create AML Experiment\nWe will track runs of the feature engineering script in a dedicated Experiment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\nexperiment_name = 'aerial-feature-engineering'\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Start and monitor a remote run\n\nWe will run a script on a single node in a docker container. The docker image will be configured and created using AML APIs.\n\nThe first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n\nYou can check the progress of a running job in multiple ways: Azure Portal, AML Jupyter Widgets, log files streaming. We will use AML Jupyter Widgets."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\n# Define the location of the dataprep script and the location for the output bottleneck files\nscript_folder = 'script'\nscript_name = 'extract.py'\noutput_dir = 'bottleneck_features'\n\npip_packages = ['h5py', \n                'pillow', \n                'tqdm', \n                'azureml-sdk[contrib]', \n                'scikit-learn', \n                'tensorflow-gpu==1.10']\n\nscript_params = {\n    '--input_data_dir': input_ds.as_download(),\n    '--output_data_dir': output_dir,\n    '--file_name': 'aerial_bottleneck_resnet50_brainwave.h5'\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                inputs=[output_ds.path(output_dir).as_upload(path_on_compute=output_dir)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Submit the run and start RunDetails widget."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\ntags = {\"Compute target\": \"AML Compute GPU\", \"DNN\": \"Brainwave ResNet50\"}\nrun = exp.submit(config=est, tags=tags)\n\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Block to wait till the run finishes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After the run, AML copied the output bottleneck files to the default datastore. You can verify it using Azure Portal."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training\nThe run has completed. The next step is to train a small Fully Connected Neural Network using engineered bottleneck features.\n\nWe will use AML feature called `Hyperdrive` to fine tune hyperparameters of the neural network. `Hyperdrive` will utilize Azure ML Compute GPU cluster to run and evaluate concurrent training jobs. After the model is fine tuned, the best version will be registered in AML Model Registry.\n\n### Create training script\n\nIn the training script, we use Tensorflow.Keras to define and train a simple fully connected neural network.\n\nThe network has one hidden layer. The input to the network is a vector of 2048 floating point numbers - the bottleneck features created in the previous step. The output layer consists of 6 units - representing six land type classes. To control overfitting the network uses a Dropout layer between the hidden layer and the output layer and L1 and L2 regularization in the output layer.\n\nThe number of units in the hidden layer, L1 and L2 values, and batch size are all tuneable hyperparameters. The Dropout ratio is fixed at 0.5.\n\nSince the bottleneck feature files are small (as compared to original image datasets) they can be loaded into memory all at once.\n\nThe trained model will be saved into the ./outputs folder. This is one of the special folders in AML. The other one is the ./logs folder. The content in these folders is automatically uploaded to the run history.\n\nThe script uses AML Run object to track two performane measures: training accuracy and validation accuracy. The metrics are captured at the end of each epoch.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "script_name = 'train.py'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.applications import resnet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\nfrom tensorflow.keras.regularizers import l1_l2\nfrom sklearn.model_selection import train_test_split\n\nfrom azureml.core import Run\n\nimport numpy as np\nimport random\nimport h5py\n\n\n# Create custom callback to track accuracy measures in AML Experiment\nclass RunCallback(tf.keras.callbacks.Callback):\n    def __init__(self, run):\n        self.run = run\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n\n\n# Define network\ndef fcn_classifier(input_shape=(2048,), units=512, classes=6,  l1=0.01, l2=0.01):\n    features = Input(shape=input_shape)\n    x = Dense(units, activation='relu')(features)\n    x = Dropout(0.5)(x)\n    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n    model = Model(inputs=features, outputs=y)\n    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Training regime\ndef train_evaluate(run):\n   \n    print(\"Loading bottleneck features\")\n    train_file_name = os.path.join(FLAGS.data_folder, FLAGS.train_file_name)\n    \n    # Load bottleneck training features and labels\n    with h5py.File(train_file_name, \"r\") as hfile:\n        features = np.array(hfile.get('features'))\n        labels = np.array(hfile.get('labels'))\n        \n    \n        \n    # Split the data into training and validation partitions   \n    X_train, X_validation, y_train, y_validation = train_test_split(features, labels,\n                                                               test_size=0.1,\n                                                               shuffle=True,\n                                                               stratify=labels)\n        \n    # Convert labels into one-hot encoded format\n    y_train = to_categorical(y_train, num_classes=6)\n    y_validation = to_categorical(y_validation, num_classes=6)\n    \n    # Create a network\n    model = fcn_classifier(input_shape=(2048,), units=FLAGS.units, l1=FLAGS.l1, l2=FLAGS.l2)\n    \n    # Create AML tracking callback\n    run_callback = RunCallback(run)\n    \n    # Start training\n    print(\"Starting training\")\n    model.fit(X_train, y_train,\n          batch_size=FLAGS.batch_size,\n          epochs=FLAGS.epochs,\n          shuffle=True,\n          validation_data=(X_validation, y_validation),\n          callbacks=[run_callback])\n          \n    # Save the trained model to outputs which is a standard folder expected by AML\n    print(\"Training completed.\")\n    os.makedirs('outputs', exist_ok=True)\n    model_file = os.path.join('outputs', 'aerial_fcnn_classifier.hd5')\n    print(\"Saving model to: {0}\".format(model_file))\n    model.save(model_file)\n    \n\nFLAGS = tf.app.flags.FLAGS\n\n# Default global parameters\ntf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\ntf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\ntf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\ntf.app.flags.DEFINE_float('l1', 0.01, \"l1 regularization\")\ntf.app.flags.DEFINE_float('l2', 0.01, \"l2 regularization\")\ntf.app.flags.DEFINE_string('data_folder', './bottleneck', \"Folder with bottleneck features and labels\")\ntf.app.flags.DEFINE_string('train_file_name', 'aerial_bottleneck_resnet50.h5', \"Training file name\")\n\ndef main(argv=None):\n    \n    # get hold of the current run\n    run = Run.get_submitted_run()\n    train_evaluate(run)\n  \n\nif __name__ == '__main__':\n    tf.app.run()\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure datastore\n\nThe bottleneck files have been uploaded to the workspace's default datastore during the previous step. We will mount the store on the nodes of the cluster.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nds = ws.get_default_datastore()\nprint(\"Using the default datastore for training data: \")\nprint(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run a test run on a single node of the cluster"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data_folder': ds.path('bottleneck_features').as_download(),\n    '--train_file_name': 'aerial_bottleneck_resnet50_brainwave.h5',\n    '--l1': 0.001,\n    '--l2': 0.001,\n    '--units': 512,\n    '--epochs': 10\n}\n\n\n#pip_packages = ['h5py','pillow', 'scikit-learn', 'tensorflow-gpu==1.10']\n\npip_packages = ['h5py', \n                'pillow', \n                'tqdm', \n                'azureml-sdk[contrib]', \n                'scikit-learn', \n                'tensorflow-gpu==1.10']\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tags = {\"Compute target\": \"BAI\", \"Run Type\": \"Test drive\"}\nrun = exp.submit(est, tags=tags)\nrun",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure `Hyperdrive`\n\nAs noted before, our network has 5 hyperparameters:\n\n- Number of units in the hidden layer\n- L1 and L2 regularization\n- mini-batch size, and\n- dropout ratio\n\nAs we have limited time to complete the lab, we are going to limit a number of hyperparameter combinations to try. We will use a fixed batch-size and dropout ratio and focus on hidden layer units and L1 and L2 regularization.\n\n*Hyperdrive* supports many strategies for sampling the hyperparameter space. In this lab, we are going to use the simplest one - grid sampling."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import *\n\nps = GridParameterSampling(\n    {\n        '--units': choice(256, 512),\n        '--l1': choice(0.001, 0.01, 0.05),\n        '--l2': choice(0.001, 0.01, 0.05)\n    }\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use **Estimator** object to configure the training job. Note how we pass the location of the bottleneck files to the estimator. The job will run on GPU VMs and as such we need to use the GPU version of Tensorflow."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data_folder': ds.path('bottleneck_features').as_download(),\n    '--train_file_name': 'aerial_bottleneck_resnet50_brainwave.h5',\n    '--epochs': 50\n}\n\n#pip_packages = ['h5py','pillow', 'scikit-learn', 'tensorflow-gpu']\n\npip_packages = ['h5py', \n                'pillow', \n                'tqdm', \n                'azureml-sdk[contrib]', \n                'scikit-learn', \n                'tensorflow-gpu==1.10']\n\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Hyperdrive* supports early termination policies to limit exploration of hyperparameter combinations that don't show promise of helping reach the target metric. This feature is especially useful when traversing large hyperparameter spaces. Since we are going to run a small number of jobs we will not apply early termination."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "policy = NoTerminationPolicy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to configure a run configuration object, and specify the primary metric as *validation_acc* that's recorded in our training runs. If you go back to visit the training script, you will notice that this value is being logged after every run. We also want to tell the service that we are looking to maximizing this value. We also set the number of total runs to 12, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htc = HyperDriveRunConfig(estimator=est, \n                          hyperparameter_sampling=ps,\n                          policy=policy,\n                          primary_metric_name='validation_acc', \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=12,\n                          max_concurrent_runs=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, let's launch the hyperparameter tuning job.\n\nThe first run takes longer as the system has to prepare and deploy a docker image with training job runtime dependencies. As long as the dependencies don't change the following runs will be much faster.\n\nHere is what's happening whie you wait.\n\n- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs. During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n\n- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically.\n\n- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n\n- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tags = {\"Training\": \"Hyperdrive\"}\n\nhdr = exp.submit(config=htc, tags=tags)\nhdr",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(hdr).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hdr.wait_for_completion(show_output=False) # specify True for a verbose log",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Find and register best model\nWhen all jobs finish, we can find out the one that has the highest accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = hdr.get_best_run_by_primary_metric()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details()['runDefinition']['Arguments']\n\nprint('Best Run Id: ', best_run.id)\nprint('\\n Validation Accuracy:', best_run_metrics['validation_acc'])\nprint('\\n Units:',parameter_values[7])\nprint('\\n L1:',parameter_values[9])\nprint('\\n L2:',parameter_values[11])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Check the output of the best run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(best_run.get_file_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Register model\nThe last step in the training script wrote the file `aerial_fcnn_classifier.hd5` in the `outputs` directory. As noted before, `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. \n\nYou can register the model so that it can be later queried, examined and deployed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = best_run.register_model(model_name='aerial_classifier', \n                                model_path='outputs/aerial_fcnn_classifier.hd5')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up resources\nBefore you move to the next step, delete the cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "compute_target.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}